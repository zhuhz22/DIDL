{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization（正则化）\n",
    "# Target：当出现过拟合问题时予以解决\n",
    "\n",
    "## L1/2 regularization(权重衰减)\n",
    "在损失函数中加入W参数的项\n",
    "<p>\n",
    "合理性?  激活函数、层数与神经元个数确定，则函数各项确定，只有参数可变。过拟合时，函数波动幅度通常大->导数大  又因为项确定，所以各项参数大----->减小参数有利于防止过拟合<p>另一观点：为了规避正则化惩罚，调小参数<=>神经元部分凋亡<=>网络简化=>防止过拟合\n",
    "<p>b才一个参数不必加入惩罚；L1(模长)在实践中的效果远不如L2(模长平方)\n",
    "\n",
    "## dropout(随机丢弃/暂退法)\n",
    "合理性？防止过分依赖某一个特定神经元，而是将一层的依赖度均摊到各个神经元上；<p>另一观点：随即丢弃后神经元个数减少，函数简化，不易产生过拟合的趋势\n",
    "\n",
    "## data augmentation（数据增广）\n",
    "\n",
    "## early-stopping   \n",
    "再在练集上训练的同时用测试集测试，当测试集上准确率开始递增时停止训练\n",
    "\n",
    "优点：相对于L2正则化，没有正则化相关的超参数，不用调参，节约算力\n",
    "\n",
    "缺点：妨碍正交化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
