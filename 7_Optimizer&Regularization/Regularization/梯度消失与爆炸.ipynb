{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度消失/爆炸\n",
    "\n",
    "## 成因\n",
    "链式求偏导时，*loss(x)=L(Wifi(Wjfj(...X)+bj)+bi) ,dL/dWij ~ XWaWbWc……dfa(A)dfb(b)dfc(C)……*<p>\n",
    "激活函数导数乘积可能极小，如激活函数为指数函数（如sigmoid、tanh,导数恒＜1）*exp(A)·exp(B)·exp(C)……* (关于层数的指数函数函数减小速率),其中ABC等为激活函数所接受的中间值;\n",
    "(通常的激活函数导数没有极大的，基本都≤1)<p>\n",
    "与此同时，Wi可能很小，乘积*WaWbWc……*会极小（关于层数的指数函数函数减小速率），从而梯度消失<p>\n",
    "而Wi可能很大(比如初始化导致的/训练出来的)，那么乘积*WaWbWc……*会极大（关于层数的指数函数函数增速），从而梯度爆炸\n",
    "\n",
    "## 危害\n",
    "过大--梯度下降时delta幅度过大，在最值点两侧震荡而难以达到最值点<p>\n",
    "过小--梯度下降时delta幅度过小，训练过慢\n",
    "\n",
    "## 解决\n",
    "既然梯度消失是由于激活函数导数恒＜1，那么换一个激活函数不就能解决问题？———用ReLU代替sigmoid作激活函数，并注意尽量避免使用sigmoid函数<p>\n",
    "初始化模型参数时将Wi设置成不大不小的值<p>\n",
    "……\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
