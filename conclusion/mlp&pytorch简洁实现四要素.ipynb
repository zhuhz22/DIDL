{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机\n",
    "在仿射变换后，每个中间神经元被激活函数处理，而后在传入下一层仿射变换<p>\n",
    "仿射变换----激活函数----仿射变换----……<p>\n",
    "Q:为什么不将原有的仿射变换直接换成不同的非线性函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "# 训练通常模式及源码剖析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dataloader=Dataloader(dataset,shuffle=True,batch_size=Batch_size)\n",
    "model=net\n",
    "loss=Loss\n",
    "optimizer=Optimizer(model.parameters(),...)\n",
    "epoch_num=10\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for X,y in dataloader:\n",
    "        py_hat=model(X)\n",
    "        loss=loss(y_hat,y) #这里必须化成零维tensor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer源码剖析：以小批量梯度下降算法为例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def optimizer.zero_grad(self):\n",
    "    for para in self.parameters():\n",
    "        para.grad.zero_()\n",
    "def optimizer.step(self):\n",
    "    for para in self.parameters():\n",
    "        para.data-=(self.lr/self.batch_size)*para.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "举例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def sgd(params, lr, batch_size):  \n",
    "   for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data!!\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y).sum()  # l是有关小批量X和y的损失\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数\n",
    "\n",
    "        # 不要忘了梯度清零\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9bb2387fa535eab90a2c1cdf6f7066082a1beb7e1accfe42d575ce58765932d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
